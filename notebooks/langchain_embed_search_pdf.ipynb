{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aa5e2fe3-1168-4bfb-9cac-56fc4c6261e2",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Embedding and Searching PDF'\n",
    "date: 2023-07-09\n",
    "categories: [LangChain]\n",
    "toc: true\n",
    "highlight-style: github\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d543e76-5d7f-4c11-90a8-adf79af4690c",
   "metadata": {},
   "source": [
    "LangChain has a `PyPDFLoader` data loader that can load a PDF file. It requires the `pypdf` package to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c85d3-e044-47e9-8c50-8cb20b0340f9",
   "metadata": {},
   "source": [
    "## Load a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c69f1f-8b77-4aaa-98ff-34863e8e46b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "pdf_pages = pdf_loader.load()\n",
    "print(f'total pages: {len(pdf_pages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284cc8a",
   "metadata": {},
   "source": [
    "The PDF file is loaded into a list of `Document`, which contains 2 fields, `page_content` and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c94e3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar\u0003\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit\u0003\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\u0003\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez\u0003y\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin\u0003z\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also con\n"
     ]
    }
   ],
   "source": [
    "print(pdf_pages[0].page_content[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605d0932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'attention_is_all_you_need.pdf', 'page': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607710f-b175-4adc-ae75-2eeb28f7058a",
   "metadata": {},
   "source": [
    "## Split the document\n",
    "\n",
    "LangChain recommends `RecursiveCharacterTextSplitter` for generic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4d5946-9e27-45fa-a0d4-ada3584c4b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents: 37\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pdf_pages)\n",
    "print(f'total documents: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6fe20c3-a14e-468a-b3ae-23aa6033324b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar\u0003\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit\u0003\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\u0003\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez\u0003y\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin\u0003z\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73728652-e24c-4b71-ace1-edf4e8c31610",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embedding the docs and use Chroma for search\n",
    "\n",
    "Use a HuggingFace embedding model to create a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda3cff3-7309-4c83-b8b2-82ff77a458cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "emb_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=emb_model,\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5979d08c-60cb-45ee-b181-a14e55707658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d02af-0950-4dd6-a2e2-b2a1f8e0bdb8",
   "metadata": {},
   "source": [
    "## Similarity search with enforced diversity\n",
    "\n",
    "Use `max_marginal_relevance_search` to achieve both relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6a9d49c-00cc-4d0f-8087-80453f045649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = 'scaled dot-product attention'\n",
    "\n",
    "results = vectordb.max_marginal_relevance_search(q, k=4, fetch_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bdbaa69-cb3e-495b-85df-c4504562ffe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q;K;V ) = softmax(QKT\n",
      "pdk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1pdk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n"
     ]
    }
   ],
   "source": [
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47ec6a-d9e0-4bc4-9880-a01c606f6451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4740f-6385-4b8b-9c6c-2ccc4b6c1974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
