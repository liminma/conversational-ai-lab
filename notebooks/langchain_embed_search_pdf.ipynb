{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c3ab93",
   "metadata": {},
   "source": [
    "# Use LangChain to embed and search PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c85d3-e044-47e9-8c50-8cb20b0340f9",
   "metadata": {},
   "source": [
    "## Load a PDF file\n",
    "\n",
    "LangChain has a PyPDFLoader for loading PDF documents. It requires the `pypdf` package to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36654f3-63e0-487f-bbef-fcfca4b5ba58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c69f1f-8b77-4aaa-98ff-34863e8e46b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages: 15\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "pdf_pages = pdf_loader.load()\n",
    "print(f'total pages: {len(pdf_pages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284cc8a",
   "metadata": {},
   "source": [
    "Each page is a `Document`, which contains 2 fields, `page_content` and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c94e3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar\u0003\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit\u0003\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\u0003\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez\u0003y\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin\u0003z\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also con\n"
     ]
    }
   ],
   "source": [
    "print(pdf_pages[0].page_content[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605d0932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'attention_is_all_you_need.pdf', 'page': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607710f-b175-4adc-ae75-2eeb28f7058a",
   "metadata": {},
   "source": [
    "## Split the document\n",
    "\n",
    "LangChain recommends `RecursiveCharacterTextSplitter` for generic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b03175-647a-4ec1-aae2-afe6267ecb4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4d5946-9e27-45fa-a0d4-ada3584c4b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents: 37\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pdf_pages)\n",
    "print(f'total documents: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fe20c3-a14e-468a-b3ae-23aa6033324b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar\u0003\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit\u0003\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\u0003\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez\u0003y\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin\u0003z\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73728652-e24c-4b71-ace1-edf4e8c31610",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embedding the docs and use Chroma for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b63261-b6b6-4c95-a9da-161e24b9e001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b503a1b-c1f7-4e3f-9b86-002086922540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=emb_model,\n",
    "    model_kwargs={'device': device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e08aa9c-054f-4fb3-a15d-1a6dc9db11e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5979d08c-60cb-45ee-b181-a14e55707658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d02af-0950-4dd6-a2e2-b2a1f8e0bdb8",
   "metadata": {},
   "source": [
    "## Similarity search with enforced diversity\n",
    "\n",
    "Use `max_marginal_relevance_search` to achieve both relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6a9d49c-00cc-4d0f-8087-80453f045649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = 'what is multi-head attention?'\n",
    "\n",
    "results = vectordb.max_marginal_relevance_search(q, k=4, fetch_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e330de1a-913c-4cbb-b6f9-d674b88e96f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47ec6a-d9e0-4bc4-9880-a01c606f6451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4740f-6385-4b8b-9c6c-2ccc4b6c1974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfChat",
   "language": "python",
   "name": "pdfchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
